{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LYvAOR2VzHmW"
   },
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis exploratorio y curación de datos\n",
    "\n",
    "### Trabajo práctico entregable - Grupo 22 - Parte 1\n",
    "\n",
    "**Integrantes:**\n",
    "- Chevallier-Boutell, Ignacio José\n",
    "- Ribetto, Federico Daniel\n",
    "- Rosa, Santiago\n",
    "- Spano, Marcelo\n",
    "\n",
    "**Seguimiento:** Meinardi, Vanesa\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1680803577547,
     "user": {
      "displayName": "Laura Minuet",
      "userId": "00787725849952937741"
     },
     "user_tz": 180
    },
    "id": "Xwdfo7z20TUK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "pd.set_option('display.max_rows', 1000) # cambiar el número de filas que se mostrarán usando display.max_rows.\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "sns.set_context('talk')\n",
    "sns.set_theme(style='white')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XY2Hl-Ma07Nn"
   },
   "source": [
    "## Lectura de los datasets\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    Parafrasear >>>>\n",
    "</span>\n",
    "\n",
    "En esta notebook, vamos a cargar el conjunto de datos de [la compentencia Kaggle](https://www.kaggle.com/dansbecker/melbourne-housing-snapshot) sobre estimación de precios de ventas de propiedades en Melbourne, Australia.\n",
    "\n",
    "Utilizaremos el conjunto de datos reducido producido por [DanB](https://www.kaggle.com/dansbecker). Hemos subido una copia a un servidor de la Universidad Nacional de Córdoba para facilitar su acceso remoto.\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    <<<< Parafrasear\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1525,
     "status": "ok",
     "timestamp": 1680803579067,
     "user": {
      "displayName": "Laura Minuet",
      "userId": "00787725849952937741"
     },
     "user_tz": 180
    },
    "id": "Vviv_sqXdR5W"
   },
   "outputs": [],
   "source": [
    "#datos kaggle\n",
    "url_kag = 'https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/melb_data.csv'\n",
    "df_kag = pd.read_csv(url_kag)\n",
    "#datos airbnb\n",
    "url_airb = 'https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/cleansed_listings_dec18.csv'\n",
    "df_airb = pd.read_csv(url_airb)\n",
    "total_ans_kag = len(df_airb) # cantidad de respuestas en el dataset kaggle\n",
    "total_ans_kag = len(df_airb) # cantidad de respuestas en el dataset airbnb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data source:\n",
    "https://www.kaggle.com/tylerx/melbourne-airbnb-open-data?select=cleansed_listings_dec18.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///sysarmy.sqlite3', echo=False)\n",
    "\n",
    "#creo por un lado el df de kaggle\n",
    "df_kag.to_sql('survey1', con=engine, if_exists=\"replace\")\n",
    "\n",
    "#creo por otro el df de airbnb\n",
    "df_airb.to_sql('survey2', con=engine, if_exists=\"replace\")\n",
    "print(df_airb.keys())\n",
    "#me quedo solo con el precio y codigo postal\n",
    "#rel_col_airb = ['price', 'zipcode']\n",
    "#df_airb2 = df_airb[rel_col_airb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cantidad de registros por ciudad:\n",
    "query_c = \"SELECT price FROM survey2 GROUP BY city\"\n",
    "#solo por ciudad y barrio:\n",
    "query_cb = \"SELECT price FROM survey2 GROUP BY city and neighborhood\"\n",
    "\n",
    "con = engine.connect()\n",
    "sql_text = text(query_cb)\n",
    "result = con.execute(sql_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 1 - SQL\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    Consigna >>>>\n",
    "</span>\n",
    "\n",
    "1. Crear una base de datos en SQLite utilizando la libreria [SQLalchemy](https://stackoverflow.com/questions/2268050/execute-sql-from-file-in-sqlalchemy).\n",
    "https://docs.sqlalchemy.org/en/14/core/engines.html#sqlite\n",
    "\n",
    "2. Ingestar los datos provistos en 'https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/melb_data.csv' en una tabla y el dataset generado en clase con datos de airbnb y sus precios por codigo postal en otra.\n",
    "\n",
    "3. Implementar consultas en SQL que respondan con la siguiente información:\n",
    "\n",
    "    - cantidad de registros totales por ciudad.\n",
    "    - cantidad de registros totales por barrio y ciudad.\n",
    "\n",
    "4. Combinar los datasets de ambas tablas ingestadas utilizando el comando JOIN de SQL  para obtener un resultado similar a lo realizado con Pandas en clase.\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    <<<< Consigna\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 2 - Pandas\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    Consigna >>>>\n",
    "</span>\n",
    "\n",
    "1. Seleccionar un subconjunto de columnas que les parezcan relevantes al problema de predicción del valor de la propiedad. Justificar las columnas seleccionadas y las que no lo fueron.\n",
    " - Eliminar los valores extremos que no sean relevantes para la predicción de valores de las propiedades.\n",
    "\n",
    " \n",
    "2. Agregar información adicional respectiva al entorno de una propiedad a partir del [conjunto de datos de AirBnB](https://www.kaggle.com/tylerx/melbourne-airbnb-open-data?select=cleansed_listings_dec18.csv) utilizado en el práctico. \n",
    "  1. Seleccionar qué variables agregar y qué combinaciones aplicar a cada una. Por ejemplo, pueden utilizar solo la columna `price`, o aplicar múltiples transformaciones como la mediana o el mínimo.\n",
    "  1. Utilizar la variable zipcode para unir los conjuntos de datos. Sólo incluir los zipcodes que tengan una cantidad mínima de registros (a elección) como para que la información agregada sea relevante.\n",
    "  2. Investigar al menos otras 2 variables que puedan servir para combinar los datos, y justificar si serían adecuadas o no. Pueden asumir que cuentan con la ayuda de anotadores expertos para encontrar equivalencias entre barrios o direcciones, o que cuentan con algoritmos para encontrar las n ubicaciones más cercanas a una propiedad a partir de sus coordenadas geográficas. **NO** es necesario que realicen la implementación.\n",
    "\n",
    "Pueden leer otras columnas del conjunto de AirBnB además de las que están en `interesting_cols`, si les parecen relevantes.\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    <<<< Consigna\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 3 - Guardado final\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    Consigna >>>>\n",
    "</span>\n",
    "\n",
    "Crear y guardar un nuevo conjunto de datos con todas las transformaciones realizadas anteriormente.\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    <<<< Consigna\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicios Opcionales\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    Consigna >>>>\n",
    "</span>\n",
    "\n",
    "1. Armar un script en python (archivo .py) [ETL](https://towardsdatascience.com/what-to-log-from-python-etl-pipelines-9e0cfe29950e) que corra los pasos de extraccion, transformacion y carga, armando una funcion para cada etapa del proceso y luego un main que corra todos los pasos requeridos.\n",
    "\n",
    "2. Armar un DAG en Apache Airflow que corra el ETL. (https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)\n",
    "\n",
    "<span style=\"color:green;font-size:30px\">\n",
    "    <<<< Consigna\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1NXWXyebR6mzAabyOPqghXpZQdyy05cRV",
     "timestamp": 1680785357097
    },
    {
     "file_id": "1OZLULlr-RkXwyhar7ED5fcXmPdgc6tGb",
     "timestamp": 1617224538047
    },
    {
     "file_id": "1xPL68eVaR0UF7XTcLU1rCMfpzuS7r6s0",
     "timestamp": 1617223947184
    },
    {
     "file_id": "1bChseZSrdGJSXfa-D2_lS0dx98wplQtD",
     "timestamp": 1614968149245
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
