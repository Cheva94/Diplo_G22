{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"zO4bRoxr2Apy"},"source":["# **Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones**\n","\n","## **Edición 2023**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uuCQ5s6ThQAD"},"source":["## Análisis exploratorio y curación de datos\n","\n","### Trabajo práctico entregable - Grupo 22 - Parte 2\n","\n","**Integrantes:**\n","- Chevallier-Boutell, Ignacio José\n","- Ribetto, Federico Daniel\n","- Rosa, Santiago\n","- Spano, Marcelo\n","\n","**Seguimiento:** Meinardi, Vanesa\n","\n","---"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OLdL2xV7hQAE"},"source":["## Librerías"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4udjxjk1EtVU","outputId":"6ad1d1f1-0b37-426c-a0c2-e0277e6b6e26"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","pd.set_option('display.max_columns', 10)\n","pd.set_option('display.max_rows', 1000)\n","pd.set_option('display.width', 1000)\n","pd.options.mode.chained_assignment = None  # default='warn'\n","\n","import seaborn as sns\n","sns.set_context('talk')\n","sns.set_theme(style='white')\n","\n","from sklearn.preprocessing import OneHotEncoder\n","import missingno as msno\n","\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.impute import IterativeImputer"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Acerca del dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En la parte 1 del entregable se seleccionaron aquellas filas y columnas que consideramos relevantes para el problema de predicción de los precios de las propiedades en Melbourn, Australia. Utilizaremos dicho conjunto de datos resultante."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"_qeFN3GnEvMk","outputId":"02e6e659-1be0-414f-a6a0-8f8298bb84d7"},"outputs":[],"source":["df = pd.read_csv('GuardadoFinal.csv').iloc[:, 1:]\n","df[:3]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 1 - Encoding"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"s-mixICN22kA"},"source":["En la mayoría de los modelos de machine learning es necesario que las variables que se utilizan para entrenarlo sean del tipo numéricas. Por este motivo, suele ser necesario encontrar algún mapeo útil que permita transformar a la variables categóricas en numéricas.\n","\n","En este caso las variables categóricas que consideramos importantes para la predicción del precio de las casas son CouncilArea, Regionname, SellerG y Type. Las 4 son variables nominales ya que no tienen un orden en sus categorías. En este sentido, consideramos que el algoritmo one-hot encoding es útil para realizar su codificación. El mismo crea una ristra de números con tantas cifras como categorías posea la variable considerada: cuando el registro pertenece a una dada categoría, se genera un 1 en dicha posición, siendo el resto de las cifras iguales a cero.\n","\n","Vamos a comenzar el proceso de codificación separando entre variables categóricas y numéricas, según lo antes mencionado. Luego, vemos la cantidad de categorías que posee cada una de las variables categóricas elegidas y el número de columnas que se creará en total luego de realizar la codificación one-hot: mapearemos las 4 columnas categóricas en 41 columnas numéricas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhgll7-IhQAG"},"outputs":[],"source":["categorical_cols = ['CouncilArea', 'Regionname', 'SellerG', 'Type']\n","numerical_cols = [x for x in df.columns if (x not in categorical_cols) and x not in ['Postcode', 'zipcode']]\n","\n","print('Cantidad de categorías para cada variable:')\n","print(df[categorical_cols].nunique())\n","print('')\n","print('Cantidad de columnas que se creará con One Hot Encoding:', df[categorical_cols].nunique().sum())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Antes de pasar a la codificación, corroboramos la presencia de datos faltantes utilizando la librería `missingno`. En el gráfico de barras vemos que en CouncilArea faltan 1355 datos, representando el 10% del total de registros. Estos registros recibirán una categória propia dentro de esta variable cuando hagamos la codificación one-hot."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(figsize=(6, 5))\n","msno.bar(df[categorical_cols], sort=\"ascending\", fontsize=12, color=\"tab:green\", ax=axs)\n","axs.set_ylim(0.8, 1)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"n8-A1qhrhQAG"},"source":["A continuación utilizamos la función OneHotEncoder de sklearn para realizar el One Hot Encoding de las variables. En el código se describe el paso a paso, pero la idea final es crear un nuevo DataFrame de Pandas con las nuevas columnas antes dichas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlrOCwUKhQAG","outputId":"97250a8b-d84d-468d-f93c-e313204f9e7c"},"outputs":[],"source":["# Nos quedamos con la columnas categóricas del DataFrame\n","features = df[categorical_cols]\n","# Creamos una lista con las categorías de cada variable categórica\n","categories = [features[column].unique() for column in features.columns]\n","# Inicializamos el enconder\n","encoder = OneHotEncoder(categories=categories)\n","# Mapeamos las categóricas a one-hot\n","encoded_features = encoder.fit_transform(features)\n","\n","# Creación de nuevas columnas para one-hot\n","feature_names = []\n","for i, column in enumerate(features.columns):\n","    for category in categories[i]:\n","        feature_names.append(f'{column}_{category}')\n","\n","encoded_df = pd.DataFrame(encoded_features.toarray(), columns=feature_names)\n","encoded_df.sample(10).T"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZMQD6WwchQAH"},"source":["Para finalizar este punto, unimos las variables numéricas originales con las categóricas codificadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWtvUQ4FhQAH"},"outputs":[],"source":["new_df = pd.concat([encoded_df, df[numerical_cols]], axis=1)\n","\n","new_df.sample(5).T"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 2 - Imputación por KNN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En este ejercicio trabajaremos sobre las variables numéricas, imputando de alguna manera en aquellos registros donde tengamos valores faltantes. Para empezar, creamos un DataFrame conteniendo las columnas de interés (todas menos aquellas que tienen información de AirBnB) y analizamos con `missingno`.\n","\n","A partir del gráfico de barras vemos que YearBuilt y BuildingArea presentan datos faltantes 60(faltan el 40% y el 48%, respectivamnete). Luego, como el nuevo DataFrame está ordenado en función de BuildingArea, el gráfico de matriz nos muestra que hay una gran correlación entre los datos faltantes en estas 2 categorías bajo análisis: la gran mayoría de datos faltantes en YearBuilt se corresponden con datos faltantes en BuildingArea. Esta idea queda clara cuando pasamos al mapa de calor, el cual mide la correlación de nulidad: qué tan fuerte la presencia (o ausencia) de una variable afecta la presencia de otra. Las variables que están completamente llenas o completamente vacías no presentan correlación significativa, así que quedan automáticamente descartadas de la gráfica. Además, la gráfica sólo completa las correlaciones en la triangular inferior. La gráfica nos da un valor de 0.8, lo cual se interpreta de la siguiente manera: es altamente probable que cada vez que un registro tiene un valor no nulo en YearBuilt también tenga un valor no nulo BuildingArea."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["numcol_airless = [x for x in numerical_cols if (x.split('_')[0] != 'airbnb')]\n","df_airless = new_df[numcol_airless].sort_values('BuildingArea')\n","\n","msno.bar(df_airless, sort=\"ascending\", fontsize=12, color=\"tab:green\", figsize=(6, 5))\n","msno.matrix(df_airless, fontsize=12, color=[0.5,0,0], figsize=(6, 5))\n","msno.heatmap(df_airless, fontsize=12, figsize=(6, 5))\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Ahora que sabemos dónde hay datos faltantes y sus posibles conexiones, vamos a pasar al tratamiento de los mismos, inclinándonos por el camino de la imputación. La técnica de imputación a utilizar se clasifica como avanzada, ya que reemplazaremos los datos faltantes por algún valor sustituto, estimado por un algortimso de  aprendizaje automático.\n","\n","Particularmente utilizaremos la imputación iterativa de sklearn (`IterativeImputer`), basada en la imputación multiple por ecuaciones encadenadas (MICE): el imputador modela cada variable con valores faltantes como una función de las otras variables, a partir de las cuales estima la imputación. La iteración la hace de manera rotatoria, generando un todos-vs-todos: en cada paso una columna es designada como output *y*, mientras que las otras columnas son tratadas como input $X$, sobre las cuales se ajusta un regresor (`estimator`) sobre las *y* conocidas para poder después predecir los valores faltantes en *y*. Esta predicción se realiza mediante regresiones múltiples sobre una muestra aleatoria de los datos y, luego, toma el promedio de los valores de regresión múltiple y usa ese valor para imputar el valor faltante. Este tipo de imputación funciona llenando los datos faltantes varias veces: todo el proceso se repite para cada variable durante `max_iter` rondas, siendo las salidas de la última ronda de iteración los resultados finales del proceso.\n","\n","El regresor a utilizar será `KNeighborsRegressor`, el cual se basa en el algoritmo de k vecinos más próximos (KNN): se basa en la *similitud de características* para predecir los valores de cualquier nuevo punto de datos. Esto significa que al nuevo punto se le asigna un valor en función de su parecido con los puntos del conjunto de entrenamiento, siendo muy útil para hacer predicciones sobre valores faltantes al encontrar los k-vecinos más cercanos a la observación con datos perdidos y luego imputarlos en función de los valores no perdidos en el *vecindario*. Observamos que utilizar `IterativeImputer` con el regresor `KNeighborsRegressor` **no** es equivalente a utilizar el imputador `KNNImputer` (también de sklearn): aunque ambos están basados en KNN, uno es un imputador en sí mismo y otro es un regresor utilizado por otro imputador. En otros términos, `KNeighborsRegressor` es el método de predicción de valores faltantes, mientras que `KNNImputer` es el método de reemplazo de valores faltantes.\n","\n","En ambas estrategias de imputación hay que realizar encoding para terminar teniendo todas variables numéricas. Además, es necesario realizar una estandarización, ya que datos con diferentes escalas introducen valores de reemplazo sesgados. Puntualmente, el imputador iterativo asume una distribución Gaussiana sobre las variables de salida, por lo que las características deberían ser normales o ser transformadas para hacerlas lo más normales posibles, mejorando el desempeño del imputador. Si bien el `KNNImputer` es rápido y fácil, no es muy preciso ni tiene cómputo para el error.\n","\n","\n","Sólo pros del iterativo: Versatil, puede utilizarse con diferentes clases de clasificadores. No dice nada sobre la normalización. \n","\n","Las imputaciones múltiples son mucho mejores que una sola imputación, ya que mide la incertidumbre de los valores perdidos de una mejor manera. \n","El otro es una imputación única"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mice_knn = df_airless.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["display(df_airless[['YearBuilt','BuildingArea']])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Instanciamos el IterativeImputer con un estimador `KNeighborsRegressor`\n","# Además le damos una semilla fija\n","mice_imputer = IterativeImputer(estimator=KNeighborsRegressor(), max_iter=10, \n","                                tol=1e-2, random_state=0)#, sample_posterior=True)\n","# iterative imputer is sensible to the tolerance and\n","# dependent on the estimator used internally.\n","# we tuned the tolerance to keep this example run with limited computational\n","# resources while not changing the results too much compared to keeping the\n","# stricter default value for the tolerance parameter.\n","# tolerances = (1e-3, 1e-1, 1e-1, 1e-2)\n","\n","mice_knn[['YearBuilt','BuildingArea']] = mice_imputer.fit_transform(mice_knn[['YearBuilt', 'BuildingArea']])\n","display(mice_knn[['YearBuilt','BuildingArea']])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["msno.bar(mice_knn, sort=\"ascending\", fontsize=12, color=\"tab:green\", figsize=(6, 5))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(1,2, figsize=(12,5))\n","\n","sns.histplot(mice_knn['YearBuilt'], ax=axs[0], binwidth=5, color='tab:orange', label='Imputado', kde=True)\n","sns.histplot(df_airless['YearBuilt'], ax=axs[0], binwidth=5, color='tab:green', label='Original', kde=True)\n","axs[0].set_ylabel(\"\")\n","axs[0].legend()\n","\n","sns.histplot(mice_knn['BuildingArea'], ax=axs[1], binwidth=20, color='tab:orange', label='Imputado', kde=True)\n","sns.histplot(df_airless['BuildingArea'], ax=axs[1], binwidth=20, color='tab:green', label='Original', kde=True)\n","axs[1].set_ylabel(\"\")\n","axs[1].legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import RobustScaler\n","\n","scaler = MinMaxScaler()\n","robust = RobustScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mice_knn_sca = df_airless.copy(deep=True)\n","mice_knn_sca[['YearBuilt','BuildingArea']] = scaler.fit_transform(mice_knn_sca[['YearBuilt', 'BuildingArea']])\n","mice_knn_sca[['YearBuilt','BuildingArea']] = mice_imputer.fit_transform(mice_knn_sca[['YearBuilt', 'BuildingArea']])\n","mice_knn_sca[['YearBuilt','BuildingArea']] = scaler.inverse_transform(mice_knn_sca[['YearBuilt', 'BuildingArea']])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mice_knn_rob = df_airless.copy(deep=True)\n","mice_knn_rob[['YearBuilt','BuildingArea']] = robust.fit_transform(mice_knn_rob[['YearBuilt', 'BuildingArea']])\n","mice_knn_rob[['YearBuilt','BuildingArea']] = mice_imputer.fit_transform(mice_knn_rob[['YearBuilt', 'BuildingArea']])\n","mice_knn_rob[['YearBuilt','BuildingArea']] = robust.inverse_transform(mice_knn_rob[['YearBuilt', 'BuildingArea']])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(1,2, figsize=(12,5))\n","\n","sns.histplot(mice_knn['YearBuilt'], ax=axs[0], binwidth=5, label='Imputado', kde=True)\n","sns.histplot(mice_knn_sca['YearBuilt'], ax=axs[0], binwidth=5, label='Escaleado', kde=True)\n","sns.histplot(mice_knn_rob['YearBuilt'], ax=axs[0], binwidth=5, label='Robusto', kde=True)\n","sns.histplot(df_airless['YearBuilt'], ax=axs[0], binwidth=5, label='Original', kde=True)\n","axs[0].set_ylabel(\"\")\n","axs[0].legend()\n","\n","sns.histplot(mice_knn['BuildingArea'], ax=axs[1], binwidth=20, label='Imputado', kde=True)\n","sns.histplot(mice_knn_sca['BuildingArea'], ax=axs[1], binwidth=20, label='Escaleado', kde=True)\n","sns.histplot(mice_knn_rob['BuildingArea'], ax=axs[1], binwidth=20, label='Robusto', kde=True)\n","sns.histplot(df_airless['BuildingArea'], ax=axs[1], binwidth=20, label='Original', kde=True)\n","axs[1].set_ylabel(\"\")\n","axs[1].legend()\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ismngxPcfoWb"},"source":["En el teórico se presentó el método `IterativeImputer` para imputar valores faltantes en variables numéricas. Sin embargo, los ejemplos presentados sólo utilizaban algunas variables numéricas presentes en el conjunto de datos. En este ejercicio, utilizaremos la matriz de datos codificada para imputar datos faltantes de manera más precisa.\n","\n","1. Agregue a la matriz obtenida en el punto anterior las columnas `YearBuilt` y `BuildingArea`.\n","2. Aplique una instancia de `IterativeImputer` con un estimador `KNeighborsRegressor` para imputar los valores de las variables. ¿Es necesario estandarizar o escalar los datos previamente?\n","3. Realice un gráfico mostrando la distribución de cada variable antes de ser imputada, y con ambos métodos de imputación."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ImjXQZUbVoKH"},"source":["Ejemplo de gráfico comparando las distribuciones de datos obtenidas con cada método de imputación."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 3 - Reducción de dimensionalidad."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NBN7-5OIxjJW"},"source":["Utilizando la matriz obtenida en el ejercicio anterior:\n","1. Aplique `PCA` para obtener $n$ componentes principales de la matriz, donde `n = min(20, X.shape[0])`. ¿Es necesario estandarizar o escalar los datos?\n","2. Grafique la varianza capturada por los primeros $n$ componentes principales, para cada $n$.\n","3. En base al gráfico, seleccione las primeras $m$ columnas de la matriz transformada para agregar como nuevas características al conjunto de datos."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 4 - Composición del resultado"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WrZTYmG_ZyDy"},"source":["Transformar nuevamente el conjunto de datos procesado en un `pandas.DataFrame` y guardarlo en un archivo.\n","\n","Para eso, será necesario recordar el nombre original de cada columna de la matriz, en el orden correcto. Tener en cuenta:\n","1. El método `OneHotEncoder.get_feature_names` o el atributo `OneHotEncoder.categories_` permiten obtener una lista con los valores de la categoría que le corresponde a cada índice de la matriz.\n","2. Ninguno de los métodos aplicados intercambia de lugar las columnas o las filas de la matriz."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfchYPgTxvQ4"},"outputs":[],"source":["## Small example\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import OneHotEncoder\n","\n","## If we process our data with the following steps:\n","categorical_cols = ['Type', 'Regionname']\n","numerical_cols = ['Rooms', 'Distance']\n","new_columns = []\n","\n","# Step 1: encode categorical columns\n","encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","X_cat = encoder.fit_transform(melb_df[categorical_cols])\n","for col, col_values in zip(categorical_cols, encoder.categories_):\n","  for col_value in col_values:\n","    new_columns.append('{}={}'.format(col, col_value))\n","print(\"Matrix has shape {}, with columns: {}\".format(X_cat.shape, new_columns))\n","\n","# Step 2: Append the numerical columns\n","X = numpy.hstack([X_cat, melb_df[numerical_cols].values])\n","new_columns.extend(numerical_cols)\n","print(\"Matrix has shape {}, with columns: {}\".format(X_cat.shape, new_columns))\n","\n","# Step 3: Append some new features, like PCA\n","pca = PCA(n_components=2)\n","pca_dummy_features = pca.fit_transform(X)\n","X_pca = numpy.hstack([X, pca_dummy_features])\n","new_columns.extend(['pca1', 'pca2'])\n","\n","## Re-build dataframe\n","processed_melb_df = pandas.DataFrame(data=X_pca, columns=new_columns)\n","processed_melb_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 5 - Documentación"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mVBLFc8PhRtW"},"source":["En un documento `.pdf` o `.md` realizar un reporte de las operaciones que realizaron para obtener el conjunto de datos final. Se debe incluir:\n","  1. Criterios de exclusión (o inclusión) de filas\n","  2. Interpretación de las columnas presentes\n","  2. Todas las transofrmaciones realizadas\n","\n","Este documento es de uso técnico exclusivamente, y su objetivo es permitir que otres desarrolladores puedan reproducir los mismos pasos y obtener el mismo resultado. Debe ser detallado pero consiso. Por ejemplo:\n","\n","```\n","  ## Criterios de exclusión de ejemplos\n","  1. Se eliminan ejemplos donde el año de construcción es previo a 1900\n","\n","  ## Características seleccionadas\n","  ### Características categóricas\n","  1. Type: tipo de propiedad. 3 valores posibles\n","  2. ...\n","  Todas las características categóricas fueron codificadas con un\n","  método OneHotEncoding utilizando como máximo sus 30 valores más \n","  frecuentes.\n","  \n","  ### Características numéricas\n","  1. Rooms: Cantidad de habitaciones\n","  2. Distance: Distancia al centro de la ciudad.\n","  3. airbnb_mean_price: Se agrega el precio promedio diario de \n","     publicaciones de la plataforma AirBnB en el mismo código \n","     postal. [Link al repositorio con datos externos].\n","\n","  ### Transformaciones:\n","  1. Todas las características numéricas fueron estandarizadas.\n","  2. La columna `Suburb` fue imputada utilizando el método ...\n","  3. Las columnas `YearBuilt` y ... fueron imputadas utilizando el \n","     algoritmo ...\n","  4. ...\n","\n","  ### Datos aumentados\n","  1. Se agregan las 5 primeras columnas obtenidas a través del\n","     método de PCA, aplicado sobre el conjunto de datos\n","     totalmente procesado.\n","```\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
