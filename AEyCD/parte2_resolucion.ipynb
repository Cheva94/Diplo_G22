{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"zO4bRoxr2Apy"},"source":["# **Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones**\n","\n","## **Edición 2023**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uuCQ5s6ThQAD"},"source":["## Análisis exploratorio y curación de datos\n","\n","### Trabajo práctico entregable - Grupo 22 - Parte 2\n","\n","**Integrantes:**\n","- Chevallier-Boutell, Ignacio José\n","- Ribetto, Federico Daniel\n","- Rosa, Santiago\n","- Spano, Marcelo\n","\n","**Seguimiento:** Meinardi, Vanesa\n","\n","---"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OLdL2xV7hQAE"},"source":["## Librerías"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4udjxjk1EtVU","outputId":"6ad1d1f1-0b37-426c-a0c2-e0277e6b6e26"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.width', 1000)\n","pd.options.mode.chained_assignment = None  # default='warn'\n","\n","import seaborn as sns\n","sns.set_context('talk')\n","sns.set_theme(style='white')\n","\n","from sklearn.preprocessing import OneHotEncoder"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Acerca del dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En la parte 1 del entregable se seleccionaron aquellas filas y columnas que consideramos relevantes para el problema de predicción de los precios de las propiedades en Melbourn, Australia. Utilizaremos dicho conjunto de datos resultante."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"_qeFN3GnEvMk","outputId":"02e6e659-1be0-414f-a6a0-8f8298bb84d7"},"outputs":[],"source":["df = pd.read_csv('GuardadoFinal.csv').iloc[:, 1:]\n","df[:3]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 1 - Encoding"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"s-mixICN22kA"},"source":["En la mayoría de los modelos de machine learning es necesario que las variables que se utilizan para entrenarlo sean del tipo numéricas. Por este motivo, suele ser necesario encontrar algún mapeo útil que permita transformar a la variables categóricas en numéricas.\n","\n","En este caso las variables categóricas que consideramos importantes para la predicción del precio de las casas son CouncilArea, Regionname, SellerG y Type. Las 4 son variables nominales ya que no tienen un orden en sus categorías. En este sentido, consideramos que el algoritmo one-hot encoding es útil para realizar su codificación. El mismo crea una ristra de números con tantas cifras como categorías posea la variable considerada: cuando el registro pertenece a una dada categoría, se genera un 1 en dicha posición, siendo el resto de las cifras iguales a cero.\n","\n","Vamos a comenzar el proceso de codificación separando entre variables categóricas y numéricas, según lo antes mencionado. Luego, vemos la cantidad de categorías que posee cada una de las variables categóricas elegidas y el número de columnas que se creará en total luego de realizar la codificación one-hot: mapearemos las 4 columnas categóricas en 41 columnas numéricas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhgll7-IhQAG"},"outputs":[],"source":["categorical_cols = ['CouncilArea', 'Regionname', 'SellerG', 'Type']\n","numerical_cols = [x for x in df.columns if (x not in categorical_cols) and x not in ['Postcode', 'zipcode']]\n","\n","print('Cantidad de categorías para cada variable:')\n","print(df[categorical_cols].nunique())\n","print('')\n","print('Cantidad de columnas que se creará con One Hot Encoding:', df[categorical_cols].nunique().sum())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"n8-A1qhrhQAG"},"source":["A continuación utilizamos la función OneHotEncoder de sklearn para realizar el One Hot Encoding de las variables. En el código se describe el paso a paso, pero la idea final es crear un nuevo DataFrame de Pandas con las nuevas columnas antes dichas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlrOCwUKhQAG","outputId":"97250a8b-d84d-468d-f93c-e313204f9e7c"},"outputs":[],"source":["# Nos quedamos con la columnas categóricas del DataFrame\n","features = df[categorical_cols]\n","# Creamos una lista con las categorías de cada variable categórica\n","categories = [features[column].unique() for column in features.columns]\n","# Inicializamos el enconder\n","encoder = OneHotEncoder(categories=categories)\n","# Mapeamos las categóricas a one-hot\n","encoded_features = encoder.fit_transform(features)\n","\n","# Creación de nuevas columnas para one-hot\n","feature_names = []\n","for i, column in enumerate(features.columns):\n","    for category in categories[i]:\n","        feature_names.append(f'{column}_{category}')\n","\n","encoded_df = pd.DataFrame(encoded_features.toarray(), columns=feature_names)\n","encoded_df.sample(10).T"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZMQD6WwchQAH"},"source":["Para finalizar este punto, unimos las variables numéricas originales con las categóricas codificadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWtvUQ4FhQAH"},"outputs":[],"source":["new_df = pd.concat([encoded_df, df[numerical_cols]], axis=1)\n","\n","new_df.sample(5).T"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 2 - Imputación por KNN"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ismngxPcfoWb"},"source":["En el teórico se presentó el método `IterativeImputer` para imputar valores faltantes en variables numéricas. Sin embargo, los ejemplos presentados sólo utilizaban algunas variables numéricas presentes en el conjunto de datos. En este ejercicio, utilizaremos la matriz de datos codificada para imputar datos faltantes de manera más precisa.\n","\n","1. Agregue a la matriz obtenida en el punto anterior las columnas `YearBuilt` y `BuildingArea`.\n","2. Aplique una instancia de `IterativeImputer` con un estimador `KNeighborsRegressor` para imputar los valores de las variables. ¿Es necesario estandarizar o escalar los datos previamente?\n","3. Realice un gráfico mostrando la distribución de cada variable antes de ser imputada, y con ambos métodos de imputación."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G4ClSr_JapCw"},"outputs":[],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.impute import IterativeImputer\n","\n","melb_data_mice = melb_df.copy(deep=True)\n","\n","mice_imputer = IterativeImputer(random_state=0, estimator=KNeighborsRegressor())\n","melb_data_mice[['YearBuilt','BuildingArea']] = mice_imputer.fit_transform(\n","    melb_data_mice[['YearBuilt', 'BuildingArea']])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ImjXQZUbVoKH"},"source":["Ejemplo de gráfico comparando las distribuciones de datos obtenidas con cada método de imputación."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMK1ktqYQTJK"},"outputs":[],"source":["mice_year_built = melb_data_mice.YearBuilt.to_frame()\n","mice_year_built['Imputation'] = 'KNN over YearBuilt and BuildingArea'\n","melb_year_build = melb_df.YearBuilt.dropna().to_frame()\n","melb_year_build['Imputation'] = 'Original'\n","data = pandas.concat([mice_year_built, melb_year_build])\n","fig = plt.figure(figsize=(8, 5))\n","g = seaborn.kdeplot(data=data, x='YearBuilt', hue='Imputation')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 3 - Reducción de dimensionalidad."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NBN7-5OIxjJW"},"source":["Utilizando la matriz obtenida en el ejercicio anterior:\n","1. Aplique `PCA` para obtener $n$ componentes principales de la matriz, donde `n = min(20, X.shape[0])`. ¿Es necesario estandarizar o escalar los datos?\n","2. Grafique la varianza capturada por los primeros $n$ componentes principales, para cada $n$.\n","3. En base al gráfico, seleccione las primeras $m$ columnas de la matriz transformada para agregar como nuevas características al conjunto de datos."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 4 - Composición del resultado"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WrZTYmG_ZyDy"},"source":["Transformar nuevamente el conjunto de datos procesado en un `pandas.DataFrame` y guardarlo en un archivo.\n","\n","Para eso, será necesario recordar el nombre original de cada columna de la matriz, en el orden correcto. Tener en cuenta:\n","1. El método `OneHotEncoder.get_feature_names` o el atributo `OneHotEncoder.categories_` permiten obtener una lista con los valores de la categoría que le corresponde a cada índice de la matriz.\n","2. Ninguno de los métodos aplicados intercambia de lugar las columnas o las filas de la matriz."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfchYPgTxvQ4"},"outputs":[],"source":["## Small example\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import OneHotEncoder\n","\n","## If we process our data with the following steps:\n","categorical_cols = ['Type', 'Regionname']\n","numerical_cols = ['Rooms', 'Distance']\n","new_columns = []\n","\n","# Step 1: encode categorical columns\n","encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","X_cat = encoder.fit_transform(melb_df[categorical_cols])\n","for col, col_values in zip(categorical_cols, encoder.categories_):\n","  for col_value in col_values:\n","    new_columns.append('{}={}'.format(col, col_value))\n","print(\"Matrix has shape {}, with columns: {}\".format(X_cat.shape, new_columns))\n","\n","# Step 2: Append the numerical columns\n","X = numpy.hstack([X_cat, melb_df[numerical_cols].values])\n","new_columns.extend(numerical_cols)\n","print(\"Matrix has shape {}, with columns: {}\".format(X_cat.shape, new_columns))\n","\n","# Step 3: Append some new features, like PCA\n","pca = PCA(n_components=2)\n","pca_dummy_features = pca.fit_transform(X)\n","X_pca = numpy.hstack([X, pca_dummy_features])\n","new_columns.extend(['pca1', 'pca2'])\n","\n","## Re-build dataframe\n","processed_melb_df = pandas.DataFrame(data=X_pca, columns=new_columns)\n","processed_melb_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 5 - Documentación"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mVBLFc8PhRtW"},"source":["En un documento `.pdf` o `.md` realizar un reporte de las operaciones que realizaron para obtener el conjunto de datos final. Se debe incluir:\n","  1. Criterios de exclusión (o inclusión) de filas\n","  2. Interpretación de las columnas presentes\n","  2. Todas las transofrmaciones realizadas\n","\n","Este documento es de uso técnico exclusivamente, y su objetivo es permitir que otres desarrolladores puedan reproducir los mismos pasos y obtener el mismo resultado. Debe ser detallado pero consiso. Por ejemplo:\n","\n","```\n","  ## Criterios de exclusión de ejemplos\n","  1. Se eliminan ejemplos donde el año de construcción es previo a 1900\n","\n","  ## Características seleccionadas\n","  ### Características categóricas\n","  1. Type: tipo de propiedad. 3 valores posibles\n","  2. ...\n","  Todas las características categóricas fueron codificadas con un\n","  método OneHotEncoding utilizando como máximo sus 30 valores más \n","  frecuentes.\n","  \n","  ### Características numéricas\n","  1. Rooms: Cantidad de habitaciones\n","  2. Distance: Distancia al centro de la ciudad.\n","  3. airbnb_mean_price: Se agrega el precio promedio diario de \n","     publicaciones de la plataforma AirBnB en el mismo código \n","     postal. [Link al repositorio con datos externos].\n","\n","  ### Transformaciones:\n","  1. Todas las características numéricas fueron estandarizadas.\n","  2. La columna `Suburb` fue imputada utilizando el método ...\n","  3. Las columnas `YearBuilt` y ... fueron imputadas utilizando el \n","     algoritmo ...\n","  4. ...\n","\n","  ### Datos aumentados\n","  1. Se agregan las 5 primeras columnas obtenidas a través del\n","     método de PCA, aplicado sobre el conjunto de datos\n","     totalmente procesado.\n","```\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
