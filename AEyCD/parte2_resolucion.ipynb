{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"zO4bRoxr2Apy"},"source":["# **Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones**\n","\n","## **Edición 2023**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uuCQ5s6ThQAD"},"source":["## Análisis exploratorio y curación de datos\n","\n","### Trabajo práctico entregable - Grupo 22 - Parte 2\n","\n","**Integrantes:**\n","- Chevallier-Boutell, Ignacio José\n","- Ribetto, Federico Daniel\n","- Rosa, Santiago\n","- Spano, Marcelo\n","\n","**Seguimiento:** Meinardi, Vanesa\n","\n","---"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OLdL2xV7hQAE"},"source":["## Librerías"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4udjxjk1EtVU","outputId":"6ad1d1f1-0b37-426c-a0c2-e0277e6b6e26"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","pd.set_option('display.max_columns', 10)\n","pd.set_option('display.max_rows', 1000)\n","pd.set_option('display.width', 1000)\n","pd.options.mode.chained_assignment = None  # default='warn'\n","\n","import seaborn as sns\n","sns.set_context('talk')\n","sns.set_theme(style='white')\n","\n","from sklearn.preprocessing import OneHotEncoder\n","import missingno as msno\n","\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.impute import IterativeImputer\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import RobustScaler\n","\n","scaler = MinMaxScaler()\n","robust = RobustScaler()\n","\n","def normalizer(data):\n","    Media = data.mean()\n","    SD = data.std()\n","    normData = (data - Media) / SD\n","\n","    return normData, Media, SD\n","\n","def denormalizer(data, m, s):\n","    return data * s + m"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Acerca del dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En la parte 1 del entregable se seleccionaron aquellas filas y columnas que consideramos relevantes para el problema de predicción de los precios de las propiedades en Melbourn, Australia. Utilizaremos dicho conjunto de datos resultante."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"_qeFN3GnEvMk","outputId":"02e6e659-1be0-414f-a6a0-8f8298bb84d7"},"outputs":[],"source":["df = pd.read_csv('GuardadoFinal.csv').iloc[:, 1:]\n","df[:3]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 1 - Encoding"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"s-mixICN22kA"},"source":["En la mayoría de los modelos de machine learning es necesario que las variables que se utilizan para entrenarlo sean del tipo numéricas. Por este motivo, suele ser necesario encontrar algún mapeo útil que permita transformar a la variables categóricas en numéricas.\n","\n","En este caso las variables categóricas que consideramos importantes para la predicción del precio de las casas son CouncilArea, Regionname, SellerG y Type. Las 4 son variables nominales ya que no tienen un orden en sus categorías. En este sentido, consideramos que el algoritmo one-hot encoding es útil para realizar su codificación. El mismo crea una ristra de números con tantas cifras como categorías posea la variable considerada: cuando el registro pertenece a una dada categoría, se genera un 1 en dicha posición, siendo el resto de las cifras iguales a cero.\n","\n","Vamos a comenzar el proceso de codificación separando entre variables categóricas y numéricas, según lo antes mencionado. Luego, vemos la cantidad de categorías que posee cada una de las variables categóricas elegidas y el número de columnas que se creará en total luego de realizar la codificación one-hot: mapearemos las 4 columnas categóricas en 41 columnas numéricas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhgll7-IhQAG"},"outputs":[],"source":["categorical_cols = ['CouncilArea', 'Regionname', 'SellerG', 'Type']\n","numerical_cols = [x for x in df.columns if (x not in categorical_cols) and x not in ['Postcode', 'zipcode']]\n","\n","print('Cantidad de categorías para cada variable:')\n","print(df[categorical_cols].nunique())\n","print('')\n","print('Cantidad de columnas que se creará con One Hot Encoding:', df[categorical_cols].nunique().sum())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Antes de pasar a la codificación, corroboramos la presencia de datos faltantes utilizando la librería `missingno`. En el gráfico de barras vemos que en CouncilArea faltan 1355 datos, representando el 10% del total de registros. Estos registros recibirán una categória propia dentro de esta variable cuando hagamos la codificación one-hot."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(figsize=(6, 5))\n","msno.bar(df[categorical_cols], sort=\"ascending\", fontsize=12, color=\"tab:green\", ax=axs)\n","axs.set_ylim(0.8, 1)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"n8-A1qhrhQAG"},"source":["A continuación utilizamos la función OneHotEncoder de sklearn para realizar el One Hot Encoding de las variables. En el código se describe el paso a paso, pero la idea final es crear un nuevo DataFrame de Pandas con las nuevas columnas antes dichas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlrOCwUKhQAG","outputId":"97250a8b-d84d-468d-f93c-e313204f9e7c"},"outputs":[],"source":["# Nos quedamos con la columnas categóricas del DataFrame\n","features = df[categorical_cols]\n","# Creamos una lista con las categorías de cada variable categórica\n","categories = [features[column].unique() for column in features.columns]\n","# Inicializamos el enconder\n","encoder = OneHotEncoder(categories=categories)\n","# Mapeamos las categóricas a one-hot\n","encoded_features = encoder.fit_transform(features)\n","\n","# Creación de nuevas columnas para one-hot\n","feature_names = []\n","for i, column in enumerate(features.columns):\n","    for category in categories[i]:\n","        feature_names.append(f'{column}_{category}')\n","\n","encoded_df = pd.DataFrame(encoded_features.toarray(), columns=feature_names)\n","encoded_df.sample(10).T"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZMQD6WwchQAH"},"source":["Para finalizar este punto, unimos las variables numéricas originales con las categóricas codificadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWtvUQ4FhQAH"},"outputs":[],"source":["new_df = pd.concat([encoded_df, df[numerical_cols]], axis=1)\n","\n","new_df.sample(5).T"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 2 - Imputación por KNN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Análisis preliminar con `missingno`"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En este ejercicio trabajaremos sobre las variables numéricas, imputando de alguna manera en aquellos registros donde tengamos valores faltantes. Para empezar, creamos un DataFrame conteniendo las columnas de interés (todas menos aquellas que tienen información de AirBnB) y analizamos con `missingno`.\n","\n","A partir del gráfico de barras vemos que YearBuilt y BuildingArea presentan datos faltantes 60(faltan el 40% y el 48%, respectivamnete). Luego, como el nuevo DataFrame está ordenado en función de BuildingArea, el gráfico de matriz nos muestra que hay una gran correlación entre los datos faltantes en estas 2 categorías bajo análisis: la gran mayoría de datos faltantes en YearBuilt se corresponden con datos faltantes en BuildingArea. Esta idea queda clara cuando pasamos al mapa de calor, el cual mide la correlación de nulidad: qué tan fuerte la presencia (o ausencia) de una variable afecta la presencia de otra. Las variables que están completamente llenas o completamente vacías no presentan correlación significativa, así que quedan automáticamente descartadas de la gráfica. Además, la gráfica sólo completa las correlaciones en la triangular inferior. La gráfica nos da un valor de 0.8, lo cual se interpreta de la siguiente manera: es altamente probable que cada vez que un registro tiene un valor no nulo en YearBuilt también tenga un valor no nulo BuildingArea."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#separamos el df entre lo que tiene información de AirBnB y lo que no. Va a ser útil después para unir todas las tablas en el problema 4.\n","\n","#primero hago el sort por BuildingArea\n","new_df = new_df.sort_values('BuildingArea')\n","\n","numcol_airless = [x for x in numerical_cols if (x.split('_')[0] != 'airbnb')]\n","df_airless = new_df[numcol_airless]#.sort_values('BuildingArea')\n","\n","numcol_air = [x for x in numerical_cols if (x.split('_')[0] == 'airbnb')]\n","df_air = new_df[numcol_air]\n","\n","\n","msno.bar(df_airless, sort=\"ascending\", fontsize=12, color=\"tab:green\", figsize=(6, 5))\n","msno.matrix(df_airless, fontsize=12, color=[0.5,0,0], figsize=(6, 5))\n","msno.heatmap(df_airless, fontsize=12, figsize=(6, 5))\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Primeras pruebas con el imputador"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Ahora que sabemos dónde hay datos faltantes y sus posibles conexiones, vamos a pasar al tratamiento de los mismos, inclinándonos por el camino de la imputación. La técnica de imputación a utilizar se clasifica como avanzada, ya que reemplazaremos los datos faltantes por algún valor sustituto, estimado por un algortimso de  aprendizaje automático.\n","\n","Particularmente utilizaremos la imputación iterativa de sklearn (`IterativeImputer`), basada en la imputación multiple por ecuaciones encadenadas (MICE): el imputador modela cada variable con valores faltantes como una función de las otras variables, a partir de las cuales estima la imputación. La iteración la hace de manera rotatoria, generando un todos-vs-todos: en cada paso una columna es designada como output *y*, mientras que las otras columnas son tratadas como input $X$, sobre las cuales se ajusta un regresor (`estimator`) sobre las *y* conocidas para poder después predecir los valores faltantes en *y*. Esta predicción se realiza mediante regresiones múltiples sobre una muestra aleatoria de los datos y, luego, toma el promedio de los valores de regresión múltiple y usa ese valor para imputar el valor faltante. Este tipo de imputación funciona llenando los datos faltantes varias veces: todo el proceso se repite para cada variable durante `max_iter` rondas, siendo las salidas de la última ronda de iteración los resultados finales del proceso.\n","\n","El regresor a utilizar será `KNeighborsRegressor`, el cual se basa en el algoritmo de k vecinos más próximos (KNN): se basa en la *similitud de características* para predecir los valores de cualquier nuevo punto de datos. Esto significa que al nuevo punto se le asigna un valor en función de su parecido con los puntos del conjunto de entrenamiento, siendo muy útil para hacer predicciones sobre valores faltantes al encontrar los k-vecinos más cercanos a la observación con datos perdidos y luego imputarlos en función de los valores no perdidos en el *vecindario*. Observamos que utilizar `IterativeImputer` con el regresor `KNeighborsRegressor` **no** es equivalente a utilizar el imputador `KNNImputer` (también de sklearn): aunque ambos están basados en KNN, uno es un imputador en sí mismo y otro es un regresor utilizado por otro imputador. En otros términos, `KNeighborsRegressor` es el método de predicción de valores faltantes, mientras que `KNNImputer` es el método de reemplazo de valores faltantes.\n","\n","En ambas estrategias de imputación hay que realizar encoding para terminar teniendo todas variables numéricas. Además, es necesario realizar una estandarización, ya que datos con diferentes escalas introducen valores de reemplazo sesgados. Puntualmente, el imputador iterativo asume una distribución Gaussiana sobre las variables de salida, por lo que las características deberían ser normales o ser transformadas para hacerlas lo más normales posibles, mejorando el desempeño del imputador. Si bien el `KNNImputer` es rápido y fácil, no es muy preciso ni tiene cómputo para el error. Por su parte, `IterativeImputer` es mucho más versátil, ya que puede utilizarse con diferentes tipos de regresores. Asimismo, las imputaciones múltiples son mucho mejores que una única imputación (como con `KNNImputer`), ya que mide la incertidumbre de los valores perdidos de una mejor manera.\n","\n","Una útima consideración antes de instanciar el imputador es que el `IterativeImputer` es sensible a la tolerancia, la cual a su vez está relacionada al regresor a utilizar. El valor de tolerancia por defecto es de 1e-3, pero la documentación recomienda usar una tolerancia del orden de 1e-2 cuando el regresor es `KNeighborsRegressor`. Comenzamos usando `max_iter=10` con `tol=5e-2` y resultó convergente para el caso sin escalear. Sin embargo, para poder converger cuando se tenía en cuenta el reescaleo tuvimos que relajar las tolerancia y extender las iteraciones, sino caíamos siempre en un `early stopping`. La única manera en que todos los casos fueran simultáneamente convergentes bajo el mismo imputador fue usando `max_iter=50` con `tol=2e-1`. Consideramos que esta no es la mejor situación ya que habrá casos donde estamos llegando a una solución subóptima. Decidimos entonces generar imputadores _ad hoc_."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Instanciamos el IterativeImputer con un estimador `KNeighborsRegressor`\n","mice_imputer = IterativeImputer(estimator=KNeighborsRegressor(), max_iter=10, \n","                                tol=5e-2, random_state=0)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Probamos el imputador con los datos *crudos*, *i.e.* sin ningún tipo de pretratamiento."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mice_knn = df_airless.copy(deep=True)\n","mice_knn[['YearBuilt','BuildingArea']] = mice_imputer.fit_transform(mice_knn[['YearBuilt', 'BuildingArea']])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Al ver el gráfico de barras de `missingno`, tenemos que efectivamente los valores faltantes han sido imputados por algún otro valor. Los histogramas nos indican qué valores han tomado las imputaciones nuevas."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["msno.bar(mice_knn, sort=\"ascending\", fontsize=12, color=\"tab:green\", figsize=(6, 5))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(1,2, figsize=(12,5))\n","\n","sns.histplot(mice_knn['YearBuilt'], ax=axs[0], binwidth=5, color='tab:orange', label='Imputado', kde=True)\n","sns.histplot(df_airless['YearBuilt'], ax=axs[0], binwidth=5, color='tab:green', label='Original', kde=True)\n","axs[0].set_ylabel(\"\")\n","axs[0].legend()\n","\n","sns.histplot(mice_knn['BuildingArea'], ax=axs[1], binwidth=20, color='tab:orange', label='Imputado', kde=True)\n","sns.histplot(df_airless['BuildingArea'], ax=axs[1], binwidth=20, color='tab:green', label='Original', kde=True)\n","axs[1].set_ylabel(\"\")\n","axs[1].legend()\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Comparación de escaleos"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Vamos a evaluar el desempeño del imputador frente a diferentes tipos de escaleo:\n","* Normalización (`normalizer`): restar la media y dividir por la desviación estándar.\n","* Escaleo simple (`MinMaxScaler`): escalea los valores al rango [0, 1].\n","* Escaleo robusto (`RobustScaler`): similar al anterior, pero usando estadística. Así es más robusto a *outliers*.\n","\n","Generamos entonces las imputaciones en función de cada escaleo."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mice_imputer = IterativeImputer(estimator=KNeighborsRegressor(), max_iter=40, \n","                                tol=1e-1, random_state=0)\n","\n","\n","Media, SD = [], []\n","# Creamos un nuevo DataFrame\n","mice_knn_norm = df_airless.copy(deep=True)\n","\n","# Normalizamos\n","mice_knn_norm['YearBuilt'], m, s = normalizer(mice_knn_norm['YearBuilt'])\n","Media.append(m)\n","SD.append(s)\n","mice_knn_norm['BuildingArea'], m, s = normalizer(mice_knn_norm['BuildingArea'])\n","Media.append(m)\n","SD.append(s)\n","\n","# Imputamos\n","mice_knn_norm[['YearBuilt','BuildingArea']] = mice_imputer.fit_transform(mice_knn_norm[['YearBuilt', 'BuildingArea']])\n","\n","# Revertimos la normalización\n","mice_knn_norm['YearBuilt'] = denormalizer(mice_knn_norm['YearBuilt'], Media[0], SD[0])\n","mice_knn_norm['BuildingArea'] = denormalizer(mice_knn_norm['BuildingArea'], Media[1], SD[1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mice_imputer = IterativeImputer(estimator=KNeighborsRegressor(), max_iter=50, \n","                                tol=2e-1, random_state=0)\n","\n","\n","# Creamos un nuevo DataFrame\n","mice_knn_sca = df_airless.copy(deep=True)\n","# Escaleamos\n","mice_knn_sca[['YearBuilt','BuildingArea']] = scaler.fit_transform(mice_knn_sca[['YearBuilt', 'BuildingArea']])\n","# Imputamos\n","mice_knn_sca[['YearBuilt','BuildingArea']] = mice_imputer.fit_transform(mice_knn_sca[['YearBuilt', 'BuildingArea']])\n","# Revertimos la transformación\n","mice_knn_sca[['YearBuilt','BuildingArea']] = scaler.inverse_transform(mice_knn_sca[['YearBuilt', 'BuildingArea']])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mice_imputer = IterativeImputer(estimator=KNeighborsRegressor(), max_iter=40, \n","                                tol=1e-1, random_state=0)\n","\n","\n","# Creamos un nuevo DataFrame\n","mice_knn_rob = df_airless.copy(deep=True)\n","# Escaleamos\n","mice_knn_rob[['YearBuilt','BuildingArea']] = robust.fit_transform(mice_knn_rob[['YearBuilt', 'BuildingArea']])\n","# Imputamos\n","mice_knn_rob[['YearBuilt','BuildingArea']] = mice_imputer.fit_transform(mice_knn_rob[['YearBuilt', 'BuildingArea']])\n","# Revertimos la transformación\n","mice_knn_rob[['YearBuilt','BuildingArea']] = robust.inverse_transform(mice_knn_rob[['YearBuilt', 'BuildingArea']])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Ahora comparamos los resultados entre el conjunto de datos original (`Original`), los datos imputados en crudo (`Imputado`) y los datos imputados luego de escalear con el escaleo simple (`Escaleado`), el escaleo robusto (`Robusto`) y la normalización (`Norm`).\n","\n","Observamos que, efectivamente, es necesario el escaleo previo, ya que la respuesta de la imputación cambia al hacerlo. Más aún: la respuesta cambia en función del escaleo que hayamos elegido. Particularmente:\n","* El imputado crudo y el escaleado simple arrojan resultados similares, siendo que el escaleado requiere 5 veces más iteraciones y una tolerancia 4 veces mayor.\n","* El escaleado robusto y el normalizado otorgan resultados similares, requieriendo la misma cantidad de iteraciones y tolerancia entre sí, siendo 4 veces más iteraciones respecto al crudo y una tolerancia 2 veces mayor."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(1,2, figsize=(12,5))\n","\n","sns.histplot(mice_knn['YearBuilt'], ax=axs[0], label='Imputado', binwidth=5, kde=True)\n","sns.histplot(mice_knn_sca['YearBuilt'], ax=axs[0], label='Escaleado', binwidth=5, kde=True)\n","sns.histplot(mice_knn_rob['YearBuilt'], ax=axs[0], label='Robusto', binwidth=5, kde=True)\n","sns.histplot(mice_knn_norm['YearBuilt'], ax=axs[0], label='Norm', binwidth=5, kde=True)\n","sns.histplot(df_airless['YearBuilt'], ax=axs[0], label='Original', binwidth=5, kde=True)\n","axs[0].set_ylabel(\"\")\n","axs[0].legend()\n","\n","sns.histplot(mice_knn['BuildingArea'], ax=axs[1], label='Imputado', binwidth=20, kde=True)\n","sns.histplot(mice_knn_sca['BuildingArea'], ax=axs[1], label='Escaleado', binwidth=20, kde=True)\n","sns.histplot(mice_knn_rob['BuildingArea'], ax=axs[1], label='Robusto', binwidth=20, kde=True)\n","sns.histplot(mice_knn_norm['BuildingArea'], ax=axs[1], label='Norm', binwidth=20, kde=True)\n","sns.histplot(df_airless['BuildingArea'], ax=axs[1], label='Original', binwidth=20, kde=True)\n","axs[1].set_ylabel(\"\")\n","axs[1].legend()\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Conclusión"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A partir de todo lo dicho creemos que sí es necesario hacer un escaleo previo a la imputación. Entre los métodos probados, nos inclinamos por el escaleo robusto aprovechando su robustez frente a *outliers* y el hecho de que sea una función ya implementada y estudiada por sklearn."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Ahora  llenamos los datos faltantes de nuestro dataset con los datos imputados:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 3 y 4 - Reducción de dimensionalidad y composición del resultado"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NBN7-5OIxjJW"},"source":["En este ejercicio deseamos encontrar las componentes principales y reducir la dimensionalidad de la matriz obtenida en el ejercicio anterior mediante la aplicación de la clase `PCA` de scikit-learn.\n","\n","Las componentes principales son *direcciones* en el espacio de nuestros datos $R^{n}$ (más específicamente son combinaciones lineales de los datos). Estos componentes o direcciones se calculan de forma tal que:\n","* Son ortogonales, es decir, no están correlacionadas.\n","* Están ordenados de acuerdo al nivel de varianza de los datos originales que representan.\n","\n","Usando estos componentes, se puede construir una proyección lineal de nuestros datos a una nueva matriz donde cada columna ahora está en la dirección de un componente principal. Luego, se selecciona un subconjunto de las primeras $d$ columnas y, por las propiedades de los componentes principales, sabemos que hemos perdido la menor cantidad de varianza de nuestros datos.\n","\n","Las componentes principales de la matriz original son computadas mediante el método `fit`, a las que luego podemos acceder a través de los atributos de la instancia `pca`. Es recomendable también estandarizar o al menos escalar la matriz original para asegurar de que todas las variables estén en las mismas unidades y ninguna tenga un peso demasiado grande.\n","\n","Vamos a comenzar aplicando `RobustScaler` a toda la matriz y haciendo un pairplot junto con el cálculo del coeficiente de correlación de Pearson para todas las características."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creamos un nuevo DataFrame para aplicar \"RobustScaler\"\n","mice_knn_rob_total = mice_knn_rob.copy(deep=True)\n","print('dataframe shape:')\n","print(mice_knn_rob_total.shape)\n","mice_knn_rob_total.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Escaleamos el df completo\n","mice_knn_rob_total = robust.fit_transform(mice_knn_rob_total)\n","mice_knn_rob_total"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Hacemos el pairplot junto con el cálculo de el coef. de Pearson\n","from scipy.stats import pearsonr\n","df_test = pd.DataFrame(mice_knn_rob_total,columns = ['Distance','Lattitude','Longtitude','YearBuilt','Landsize','BuildingArea','Rooms','Bedroom2','Bathroom','Price'])\n","def corrfunc(x, y, ax=None, **kws):\n","    \"\"\"Plot the correlation coefficient in the top left hand corner of a plot.\"\"\"\n","    r, _ = pearsonr(x, y)\n","    ax = ax or plt.gca()\n","    ax.annotate(f'ρ = {r:.2f}', xy=(.1, .9), xycoords=ax.transAxes)\n","\n","g = sns.pairplot(data=df_test)\n","g.map_lower(corrfunc)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Vemos que `Rooms` y `Bedroom2` son las características más correlacionadas: $\\rho = 0.95$.\n","\n","Ahora aplicamos PCA sin buscar reducir la dimensión de la matriz original (esto es, no hay pérdida de información).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=None)\n","pca.fit(mice_knn_rob_total)\n","print(\"Componentes principales:\")\n","print(pca.components_)\n","\n","# Llevamos la matriz al espacio de componentes principales\n","mice_knn_rob_total_pca = pca.transform(mice_knn_rob_total)\n","\n","print(\"Dimensión de la matriz transformada:\")\n","print(mice_knn_rob_total_pca.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Las componentes principales están ordenadas de mayor a menor en el sentido de información que aportan. La fracción de varianza explicada es una cantidad que mide la proporción de varianza en los datos que es explicada por cada componente principal. En otras palabras, esta cantidad nos permite ver cuánto aporta cada componente. También es interesante analizar cuánto es el acumulado de todas las componentes para poder establecer un criterio como, por ejemplo, quedarse con el número mínimo de componentes que aseguren un 90% de la información original."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Varianza explicada:\")\n","print(pca.explained_variance_)\n","print()\n","print(\"Razón de varianza explicada:\")\n","evr = pca.explained_variance_ratio_\n","print(evr)\n","print()\n","print(\"porcentaje de la variable explicada:\")\n","print(np.sum(evr))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Vamos a graficar la fracción de varianza que aporta cada componente y la información acumulada."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n","\n","ax[0].plot(range(1, len(evr) + 1), evr, '.-', markersize = 10)\n","ax[0].set_ylabel('Fracción de varianza explicada')\n","ax[0].set_xlabel('Número de componente principal')\n","\n","varianza_acumulada = np.cumsum(evr)\n","ax[1].plot(range(1, len(evr) + 1), varianza_acumulada, '.-', markersize = 10)\n","ax[1].set_ylabel('Fracción acumulada de varianza explicada')\n","ax[1].set_xlabel('Cantidad de componentes principales')\n","\n","fig.subplots_adjust(top=1.05)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En el segundo gráfico vemos que, reteniendo solamente 6 componentes principales, mantenemos aproximadamente el 90% de la información original.\n","\n","Veamos qué nos dicen las primeras 2 componentes (las cuales contienen más del 50% de la información), con el fin de luego realizar un análisis gráfico en 2 dimensiones. Es decir, queremos usar PCA para visualizar los datos en un espacio de dimensión reducida."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Features: ','Distance','Lattitude','Longtitude','YearBuilt','Landsize','BuildingArea','Rooms','Bedroom2','Bathroom','Price')\n","print('PCA1 = {}'.format(pca.components_[0]))\n","print('PCA2 = {}'.format(pca.components_[1]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En la primera componente las características que más peso tienen son `Rooms` y `Bedroom2`, las cuales tienen un peso muy parecido. Esto tiene sentido ya que como vimos previamente en el pairplot, está variables tienen un coeficiente de correlación alto.\n","\n","En la segunda componente la característica que más importa es `YearBuilt`.\n","\n","Veamos ahora los datos en el espacio de estas dos primeras componentes principales:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = ['Distance','Lattitude','Longtitude','YearBuilt','Landsize','BuildingArea','Rooms','Bedroom2','Bathroom','Price']\n","features_pc = pca.components_.T\n","\n","fig, ax = plt.subplots(figsize = (12, 12))\n","\n","# Hacemos un scatter de los datos en las dos primeras componentes\n","ax.scatter(mice_knn_rob_total_pca[:,0], mice_knn_rob_total_pca[:,1], alpha = 0.65)\n","\n","# Hacemos el grafico de las flechas indicando las direcciones de los features originales\n","sf = 5 # Factor de escala para agrandar las flechas\n","\n","for i in range(len(features)):\n","\n","  ax.arrow(0, 0, sf * features_pc[i][0], sf * features_pc[i][1], width = 0.1, color = 'g', alpha = 0.5)\n","  ax.text(sf * features_pc[i][0], sf * features_pc[i][1], s = features[i], fontdict= {'color': 'k', 'size': 10})\n","\n","ax.set_xlabel('Primera componente principal')\n","ax.set_ylabel('Segunda componente principal')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Como era de esperar, vemos que `Rooms` y `Bedroom2` son casi paralelas a la primera componente principal, mientras que `YearBuilt` es casi paralela a la segunda componente principal.\n","\n","Finalmente, agregaremos las 6 primeras componentes principales al conjunto de datos original. Para ello primeros convertimos el array transformado a un dataframe nuevo:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["names = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']\n","df_pca = pd.DataFrame(mice_knn_rob_total_pca,columns = names)\n","print(df_pca.shape)\n","df_pca.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Y ahora unimos las primeras 6 columnas al dataframe original, y sumamos los datos de la tabla de AirBnB para tener la composición final del resultado del ejercicio 4:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_join1 = mice_knn_rob.copy()\n","df_join2 = df_pca.copy() \n","\n","for name in ['PC1','PC2','PC3','PC4','PC5','PC6']:\n","    df_join1[name] = df_join2[name].values\n","\n","df_join1\n","\n","final_df = pd.concat([df_join1, df_air], axis=1)\n","\n","\n","print(final_df.keys())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","# Ejercicio 5 - Documentación"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mVBLFc8PhRtW"},"source":["En un documento `.pdf` o `.md` realizar un reporte de las operaciones que realizaron para obtener el conjunto de datos final. Se debe incluir:\n","  1. Criterios de exclusión (o inclusión) de filas\n","  2. Interpretación de las columnas presentes\n","  2. Todas las transofrmaciones realizadas\n","\n","Este documento es de uso técnico exclusivamente, y su objetivo es permitir que otres desarrolladores puedan reproducir los mismos pasos y obtener el mismo resultado. Debe ser detallado pero consiso. Por ejemplo:\n","\n","```\n","  ## Criterios de exclusión de ejemplos\n","  1. Se eliminan ejemplos donde el año de construcción es previo a 1900\n","\n","  ## Características seleccionadas\n","  ### Características categóricas\n","  1. Type: tipo de propiedad. 3 valores posibles\n","  2. ...\n","  Todas las características categóricas fueron codificadas con un\n","  método OneHotEncoding utilizando como máximo sus 30 valores más \n","  frecuentes.\n","  \n","  ### Características numéricas\n","  1. Rooms: Cantidad de habitaciones\n","  2. Distance: Distancia al centro de la ciudad.\n","  3. airbnb_mean_price: Se agrega el precio promedio diario de \n","     publicaciones de la plataforma AirBnB en el mismo código \n","     postal. [Link al repositorio con datos externos].\n","\n","  ### Transformaciones:\n","  1. Todas las características numéricas fueron estandarizadas.\n","  2. La columna `Suburb` fue imputada utilizando el método ...\n","  3. Las columnas `YearBuilt` y ... fueron imputadas utilizando el \n","     algoritmo ...\n","  4. ...\n","\n","  ### Datos aumentados\n","  1. Se agregan las 5 primeras columnas obtenidas a través del\n","     método de PCA, aplicado sobre el conjunto de datos\n","     totalmente procesado.\n","```\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
