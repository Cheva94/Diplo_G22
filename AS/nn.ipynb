{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8ATtHkdth7t"
   },
   "source": [
    "# DiploDatos Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yFhBWSnYth74"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "# from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def rmse( modeldata , targetdata ) :\n",
    "    return np.sqrt( np.mean( (modeldata.flatten() - targetdata.flatten()) ** 2 ) )\n",
    "\n",
    "def bias( modeldata , targetdata ) :\n",
    "    return np.mean( modeldata.flatten() - targetdata.flatten() )\n",
    "\n",
    "def corr_P( modeldata , targetdata ) :    \n",
    "    return np.corrcoef( modeldata.flatten() , targetdata.flatten() )[0,1]\n",
    "\n",
    "def corr_S( modeldata , targetdata ) :    \n",
    "    return spearmanr( modeldata.flatten() , targetdata.flatten() )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xvo9-M0Gth78"
   },
   "source": [
    "## Leer el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0VfCzaLth79"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhtwOHy9th7-"
   },
   "source": [
    "Cargamos los datos de entrenamiento que vamos a utilizar para generar nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ktnJ_L6Sth7_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95000, 10)\n",
      "             patient           age  hypertension  heart_disease           bmi  \\\n",
      "count   95000.000000  95000.000000  95000.000000   95000.000000  95000.000000   \n",
      "mean    50016.501389     41.935269      0.075074       0.039463     27.320879   \n",
      "std     28868.357071     22.514788      0.263512       0.194695      6.626335   \n",
      "min         1.000000      0.080000      0.000000       0.000000     10.010000   \n",
      "25%     25021.750000     24.000000      0.000000       0.000000     23.650000   \n",
      "50%     50024.000000     43.000000      0.000000       0.000000     27.320000   \n",
      "75%     75024.250000     60.000000      0.000000       0.000000     29.580000   \n",
      "max    100000.000000     80.000000      1.000000       1.000000     95.690000   \n",
      "\n",
      "        HbA1c_level  blood_glucose_level      diabetes  \n",
      "count  95000.000000         95000.000000  95000.000000  \n",
      "mean       5.527659           138.070537      0.085074  \n",
      "std        1.070261            40.739962      0.278993  \n",
      "min        3.500000            80.000000      0.000000  \n",
      "25%        4.800000           100.000000      0.000000  \n",
      "50%        5.800000           140.000000      0.000000  \n",
      "75%        6.200000           159.000000      0.000000  \n",
      "max        9.000000           300.000000      1.000000  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/diabetes_prediction_dataset_train-labeled.csv')\n",
    "print(df.shape)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVh8RyBwth8I"
   },
   "source": [
    "#### Separamos a las columnas en numéricas y categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bLrwkitvth8I"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['gender', 'smoking_history']\n",
    "num_cols = [x for x in df.columns if x not in cat_cols and x not in ['patient', 'diabetes']]\n",
    "# En las columnas numéricas quitamos la columna \"patient\" que contiene el id de los pacientes y \"diabetes\" que es la variable target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aohiv4jXth8M"
   },
   "source": [
    "#### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uO9avt-1th8M"
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['patient', 'diabetes'])\n",
    "y = df['diabetes']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-0Tc87Udth8N"
   },
   "outputs": [],
   "source": [
    "#Cargo el pipeline:\n",
    "pipeline = joblib.load('pipeline.pkl')\n",
    "# Fiteo el pipeline\n",
    "x_train_transformed = pipeline.fit_transform(x_train)\n",
    "x_test_transformed = pipeline.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acá empiezo con la red neuronal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino una clase para generar datos que pueda leer el dataloader de torch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class set_up_data(Dataset):\n",
    "    \"Para utilizarse con el DataLoader de PyTorch\"\n",
    "    #nota: las series de pandas pueden tirar error en las keys, usar arrays de numpy o tensores de torch\n",
    "    def __init__(self,data,scaling='norm'):\n",
    "        self.x_data = np.array(data['x']).T\n",
    "        self.y_data = np.array(data['y'])\n",
    "        self.xmin, self.xmax = np.amin(self.x_data,axis=1), np.amax(self.x_data,axis=1)\n",
    "        \n",
    "        if scaling=='norm':\n",
    "            self.scaling_mtd = self.Norm\n",
    "        elif scaling=='01':\n",
    "            self.scaling_mtd = self.ScaleTo01\n",
    "        \n",
    "        self.x_data = self.scaling_mtd( self.x_data, self.xmin, self.xmax)\n",
    "        self.y_data = self.class_data(self.y_data)\n",
    "\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        x = torch.tensor(self.x_data[:,index], dtype=torch.float)\n",
    "        y = torch.tensor(self.y_data[:,index], dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denoto el numero total de muestras\"\n",
    "        return self.y_data.shape[1]\n",
    "    \n",
    "    def Norm(self, data, datamin, datamax):\n",
    "        #Normalizacion [0,1]\n",
    "        return (data-datamin[:,np.newaxis])/(datamax[:,np.newaxis]-datamin[:,np.newaxis])\n",
    "    \n",
    "    def ScaleTo01(self, data, datamin, datamax):\n",
    "        return data/datamax[:,np.newaxis]\n",
    "    \n",
    "    \n",
    "    def denorm(self, data, datamin, datamax):\n",
    "        return (data)*(datamax[:,np.newaxis]-datamin[:,np.newaxis])+datamin[:,np.newaxis]\n",
    "    \n",
    "        \n",
    "    def class_data(self,arr):\n",
    "\n",
    "        \"\"\"\n",
    "        transforma los target a arrays de dimensión (ndat,2)\n",
    "        donde y[:,0]=1 si es diabetes de tipo 0 y 0 si no,\n",
    "        y y[:,1]=1 si es diabetes de tipo 1 y 0 si no.\n",
    "        \"\"\"\n",
    "        \n",
    "        arr_out = np.zeros((2,len(arr)))\n",
    "\n",
    "        for i in range(len(arr)):\n",
    "            if arr[i] == 0:\n",
    "                arr_out[0,i] = 1\n",
    "            else:\n",
    "                arr_out[1,i] = 1\n",
    "    \n",
    "        return arr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fijo semilla\n",
    "\n",
    "def define_seed(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "define_seed(seed=100)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armo el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNN(nn.Module):\n",
    "\n",
    "    def __init__(self,dims):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_features = dims[0], out_features = dims[1], bias = True)\n",
    "        self.linear2 = nn.Linear(in_features = dims[1], out_features = dims[2], bias = True)\n",
    "        self.linear3 = nn.Linear(in_features = dims[2], out_features = dims[3], bias = True)\n",
    "        self.linear4 = nn.Linear(in_features = dims[3], out_features = dims[4], bias = True)\n",
    "\n",
    "        self.dims = dims\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,self.dims[0])\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = x.view(-1,self.dims[1])\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = x.view(-1,self.dims[1])\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = x.view(-1,self.dims[2])\n",
    "        x = self.linear4(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.xavier_normal_(m.weight)\n",
    "                    torch.nn.init.constant_(m.bias, 1)\n",
    "\n",
    "nlayers = [13, 260 ,260,260,2]\n",
    "model = LinearNN(nlayers)\n",
    "\n",
    "#Guardamos al modelo que definimos previamente\n",
    "model.to(device) #Cargamos en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametros\n",
    "batch_size= len(y_test)\n",
    "max_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "#cargo los datos\n",
    "train_data = {'x':x_train_transformed, 'y':y_train}\n",
    "test_data = {'x':x_test_transformed, 'y':y_test}\n",
    "\n",
    "ratio_1_0_train = len([i for i in y_train if i==1])/len(y_train)\n",
    "ratio_1_0_test = len([i for i in y_test if i==1])/len(y_test)\n",
    "\n",
    "train_subset = set_up_data(train_data, scaling='norm')\n",
    "test_subset = set_up_data(test_data, scaling='norm')\n",
    "\n",
    "dataloader_train = DataLoader(train_subset, batch_size = batch_size, shuffle=False) \n",
    "dataloader_test  = DataLoader(test_subset , batch_size=len(y_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "px-yTpRKth8R"
   },
   "outputs": [],
   "source": [
    "count_0 = len([i for i in y_train if i==0])\n",
    "count_1 = len([i for i in y_train if i==1])\n",
    "\n",
    "counts = [count_0,count_1]\n",
    "\n",
    "#La función de costo va aestar ponderada por el inverso de la cantidad de elementos de cada clase, \n",
    "#para favorecer la diabetes tipo 1, que es mucho menos recurrente\n",
    "\n",
    "ww = 1./np.array(counts)\n",
    "ww_norm = ww/np.sum(ww) #normalizo\n",
    "\n",
    "#CrossEntropyLoss para problemas de clasificación\n",
    "Loss = nn.CrossEntropyLoss(weight=torch.as_tensor(ww_norm))\n",
    "\n",
    "#Definimos el optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entreno la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m RMSE, BIAS, Corr_P, Corr_S \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m      3\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mmax_epochs\u001b[49m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#print('Epoca: '+ str(epoch+1) + ' de ' + str(max_epochs) )\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#Entrenamiento del modelo        \u001b[39;00m\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m#Esto le dice al modelo que se comporte en modo entrenamiento.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     sum_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "#Listas donde guardamos loss de entrenamiento, y para el de validación la loss y las métricas de evaluación.\n",
    "RMSE, BIAS, Corr_P, Corr_S = [], [], [], []\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    #print('Epoca: '+ str(epoch+1) + ' de ' + str(max_epochs) )\n",
    "    \n",
    "    #Entrenamiento del modelo        \n",
    "    model.train()  #Esto le dice al modelo que se comporte en modo entrenamiento.\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    batch_counter = 0\n",
    "\n",
    "    # Iteramos sobre los minibatches. \n",
    "    for inputs, target in dataloader_train :\n",
    "        #Enviamos los datos a la memoria.\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs).squeeze()\n",
    "\n",
    "        loss = Loss(outputs.float(), target.float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_counter += 1\n",
    "        sum_loss = sum_loss + loss.item()\n",
    "\n",
    "    #Calculamos la loss media sobre todos los minibatches \n",
    "    loss_train.append( sum_loss / batch_counter )\n",
    "\n",
    "    model.eval()   #Esto le dice al modelo que lo usaremos para evaluarlo (no para entrenamiento)\n",
    "\n",
    "    #Calculamos la función de costo para la muestra de testing.\n",
    "    input_test, target_test = next(iter(dataloader_test))\n",
    "    input_test, target_test = input_test.to(device) , target_test.to(device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_test = model(input_test).squeeze()\n",
    "\n",
    "    #Calculo de la loss de la epoca\n",
    "    print('Loss Train: ', str(loss_train[epoch]))\n",
    "    #print('Loss Val:   ', str(loss_val[epoch]))\n",
    "\n",
    "    ###################################\n",
    "\n",
    "    #Calculo de metricas RMSE, BIAS, Correlacion de Pearson y Spearman\n",
    "    Corr_P.append(corr_P(output_test, target_test))\n",
    "    Corr_S.append(corr_S(output_test, target_test)) \n",
    "    \n",
    "plt.plot(loss_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hago la prediccion sobre el conjunto de testing\n",
    "model.eval()   #Esto le dice al modelo que lo usaremos para evaluarlo (no para entrenamiento)\n",
    "\n",
    "input_test, target_test = next(iter(dataloader_test))\n",
    "input_test, target_test = input_test.detach().to(device) , target_test.detach().to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_test = model( input_test )\n",
    "\n",
    "#transformo outputs probabilisticos a etiquetas. La mayor probabilidad tiene un 1, el resto cero\n",
    "output_test = np.array(output_test)\n",
    "output_test0 = np.zeros(output_test.shape)\n",
    "for i in range(output_test.shape[0]):\n",
    "    max = np.argmax(output_test[i])\n",
    "    output_test0[i,max] = 1\n",
    "print(output_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import calibration as cal\n",
    "#from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "#Matriz de confusion\n",
    "plt.clf()\n",
    "cmatrix=sklearn.metrics.confusion_matrix( np.argmax(target_test,axis=1) , np.argmax(output_test,axis=1) , normalize=\"true\")\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(cmatrix)\n",
    "disp.plot()\n",
    "plt.savefig(\"tmp/conf_mat\")\n",
    "\n",
    "fig ,ax = plt.subplots(1,1)\n",
    "disp = cal.CalibrationDisplay.from_predictions( target_test[:,0], output_test[:,0], n_bins=10, name = \"lluvia severa\", ax=ax )\n",
    "ax.set_title('Diagrama de confiabilidad')\n",
    "ax.set_xlabel('Probabilidad de diabetes tipo 0')\n",
    "ax.set_ylabel('Frecuencia observada - diabetes tipo 0')\n",
    "fig.savefig(\"tmp/diag_conf_lig\")\n",
    "\n",
    "fig ,ax = plt.subplots(1,1)\n",
    "disp = cal.CalibrationDisplay.from_predictions( target_test[:,1], output_test[:,1], n_bins=10, name = \"lluvia moderada\", ax=ax )\n",
    "ax.set_title('Diagrama de confiabilidad')\n",
    "ax.set_xlabel('Probabilidad de diabetes tipo 1')\n",
    "ax.set_ylabel('Frecuencia observada - diabetes tipo 1')\n",
    "fig.savefig(\"tmp/diag_conf_mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zauBlH-th8Y"
   },
   "outputs": [],
   "source": [
    "Y_test = test_df.diabetes\n",
    "X_test = test_df.drop(columns=['patient','diabetes'])\n",
    "PatientId_test = test_df['patient']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5qL4scPth8Z"
   },
   "outputs": [],
   "source": [
    "X_test_transformed = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df9HDTVsth8Z"
   },
   "outputs": [],
   "source": [
    "# Para obtener el nombre de las columnas creadas a partir del OneHotEncoder es necesario acceder al mismo de esta manera:\n",
    "pipeline.transformers_[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfiOIMdfth8a"
   },
   "outputs": [],
   "source": [
    "# Con el método get_features_names_out se puede obtener el nombre de las columnas creadas\n",
    "pipeline.transformers_[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBJ3z6wMth8a"
   },
   "outputs": [],
   "source": [
    "cols = pipeline.transformers_[0][1].get_feature_names_out().tolist() + num_cols\n",
    "X_test_transformed = pd.DataFrame(X_test_transformed, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVyfSU1Pth8b"
   },
   "source": [
    "Generamos la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54il-gQAth8b"
   },
   "outputs": [],
   "source": [
    "test_id = PatientId_test\n",
    "test_pred = np.int64(xgb.predict(X_test_transformed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEZTLtfrth8b"
   },
   "source": [
    "Con el resultado predicho tenemos que generar el archivo `.csv` para subir a la competencia de kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKCwi6OXth8c"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(list(zip(test_id, test_pred)), columns=[\"patient\", \"diabetes\"])\n",
    "submission.to_csv(\"sample_submission.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEIBuXVBth8c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b65fc4380ac725e50a330b268a227bbdbe91bddfffbf68e5f7ce9848a2b8d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
